# Profile  
**Lee Jae Uk**  
Developer / Algorithm Learner  
---
## 🧭 Summary  
- 알고리즘 학습과 함께 AI 및 백엔드 개발 역량을 키우고 있습니다.  
- FastAPI와 ONNX 기반 실무 역량을 쌓고 있으며,  
  이론과 실습을 병행하며 깊이 있는 개발자로 성장하고자 노력합니다.  
- 최신 AI 논문을 꾸준히 읽고 구현에 적용해보며, 개념 정리와 실험을 병행합니다.
---
## 🛠 Tech Stack
**Languages**  
🐍 Python | ☕ Java | ⚡ C | ➕ C++ | 🟨 JavaScript  

**Backend**  
🍃 Spring Boot | ⚡ FastAPI | 🟢 Node.js | 🐍 Django  

**AI/ML**  
🔥 PyTorch | 🧠 TensorFlow | 🤗 HuggingFace Transformers | 🚀 ONNX  

**Infra & Tools**  
🐧 Linux | 📝 Git | 🐳 Docker | 🌐 Nginx  

**Vector Search & MLOps**  
🦜 LangChain | 🎨 ChromaDB | 🔍 FAISS | 📊 Vector DB  
---
## 📚 Currently Studying
- ⚡ FastAPI를 활용한 REST API 설계 및 백엔드 구현  
- 🚀 ONNX로 모델 추론 최적화 및 경량화 실험  
- 🍃 Spring Boot MVC 구조 및 🍃 MongoDB 기반 인증 시스템  
- 🎤 Whisper 기반 STT 모델 파인튜닝 및 🔍 RAG 실습  
---
## 📖 Papers I've Read
**Foundation Models & Architecture**
- 📄 [Attention Is All You Need (Vaswani et al., NeurIPS 2017)](https://arxiv.org/abs/1706.03762)  
  → Transformer 구조의 원조 논문  
- 📄 [BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., NAACL 2019)](https://arxiv.org/abs/1810.04805)  
  → 양방향 언어 모델의 혁신적 접근  
- 📄 [GPT-2: Language Models are Unsupervised Multitask Learners (Radford et al., 2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  
  → 대규모 언어 모델의 범용성 입증  
- 📄 [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)  
  → BERT 최적화 및 성능 개선 연구  

**🔍 Retrieval-Augmented Generation (RAG) & Knowledge Systems** 
- 📄 [Retrieval-Augmented Generation for Knowledge-Intensive NLP (Lewis et al., NeurIPS 2020)](https://arxiv.org/abs/2005.11401)  
  → RAG 구조 정의 및 Dense Passage Retrieval 도입  
- 📄 [Dense Passage Retrieval for Open-Domain Question Answering (Karpukhin et al., EMNLP 2020)](https://arxiv.org/abs/2004.04906)  
  → 밀집 벡터 기반 정보 검색 시스템  
- 📄 [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction (Khattab et al., SIGIR 2020)](https://arxiv.org/abs/2004.12832)  
  → 효율적인 문서 검색을 위한 늦은 상호작용 모델  
- 📄 [FiD: Fusion-in-Decoder for Knowledge-Intensive NLP Tasks (Izacard & Grave, NeurIPS 2021)](https://arxiv.org/abs/2007.01282)  
  → 디코더에서 다중 문서 융합을 통한 지식 집약적 NLP  
- 📄 [REALM: Retrieval-Augmented Language Model Pre-Training (Guu et al., ICML 2020)](https://arxiv.org/abs/2002.08909)  
  → 사전 훈련 단계에서 검색 기능 통합  
- 📄 [RAG-Token vs RAG-Sequence: End-to-End Retrieval Approaches (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)  
  → RAG의 두 가지 생성 방식 비교 분석  
- 📄 [RETRO: Improving Language Models by Retrieving from Trillions of Tokens (Borgeaud et al., ICML 2022)](https://arxiv.org/abs/2112.04426)  
  → 대규모 토큰 검색을 통한 언어 모델 성능 향상  
- 📄 [Internet-Augmented Dialogue Generation (Komeili et al., ICML 2022)](https://arxiv.org/abs/2107.07566)  
  → 인터넷 검색 기반 대화 생성 시스템  
- 📄 [WebGPT: Browser-assisted question-answering with human feedback (Nakano et al., 2021)](https://arxiv.org/abs/2112.09332)  
  → 웹 브라우징 기반 질의응답 시스템  
- 📄 [LaMDA: Language Models for Dialog Applications (Thoppilan et al., 2022)](https://arxiv.org/abs/2201.08239)  
  → 외부 지식 활용 대화 모델  
- 📄 [Toolformer: Language Models Can Teach Themselves to Use Tools (Schick et al., NeurIPS 2023)](https://arxiv.org/abs/2302.04761)  
  → 언어 모델이 스스로 도구 사용법 학습  
- 📄 [ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., ICLR 2023)](https://arxiv.org/abs/2210.03629)  
  → 추론과 행동을 결합한 언어 모델  
- 📄 [Langchain-Chatchat: Local Knowledge Base Q&A with RAG (2023)](https://github.com/chatchat-space/Langchain-Chatchat)  
  → 로컬 지식베이스 기반 RAG 구현  
- 📄 [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval (Sarthi et al., 2024)](https://arxiv.org/abs/2401.18059)  
  → 트리 구조 기반 재귀적 추상화 검색 시스템  
- 📄 [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection (Asai et al., 2023)](https://arxiv.org/abs/2310.11511)  
  → 자기 반성을 통한 RAG 성능 향상  
- 📄 [HyDE: Precise Zero-Shot Dense Retrieval without Relevance Labels (Gao et al., ACL 2023)](https://arxiv.org/abs/2212.10496)  
  → 가상 문서 생성을 통한 제로샷 검색  
- 📄 [RAFT: Adapting Language Model to Domain Specific RAG (Zhang et al., 2024)](https://arxiv.org/abs/2403.10131)  
  → 도메인 특화 RAG 적응 기법  
- 📄 [GraphRAG: Unlocking LLM discovery on narrative private data (Edge et al., 2024)](https://arxiv.org/abs/2404.16130)  
  → 그래프 구조를 활용한 RAG 시스템  
- 📄 [MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries (Tang et al., 2024)](https://arxiv.org/abs/2401.15391)  
  → 다단계 추론이 필요한 RAG 벤치마크  
- 📄 [Lost in the Middle: How Language Models Use Long Contexts (Liu et al., 2023)](https://arxiv.org/abs/2307.03172)  
  → 긴 컨텍스트에서의 정보 활용 패턴 분석  
- 📄 [FLARE: Active Retrieval Augmented Generation (Jiang et al., 2023)](https://arxiv.org/abs/2305.06983)  
  → 능동적 검색 기반 생성 모델  
- 📄 [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (Jeong et al., 2024)](https://arxiv.org/abs/2403.14403)  
  → 질문 복잡도에 따른 적응형 RAG  
- 📄 [Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models (Yu et al., 2023)](https://arxiv.org/abs/2311.09210)  
  → 노트 체인 방식을 통한 RAG 견고성 향상  

**📊 Vector Search & Embedding Optimization**
- 📄 [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers & Gurevych, EMNLP 2019)](https://arxiv.org/abs/1908.10084)  
  → 문장 임베딩 최적화를 위한 SBERT  
- 📄 [SimCSE: Simple Contrastive Learning of Sentence Embeddings (Gao et al., EMNLP 2021)](https://arxiv.org/abs/2104.08821)  
  → 대조 학습 기반 문장 임베딩  
- 📄 [E5: Text Embeddings by Weakly-Supervised Contrastive Pre-training (Wang et al., 2022)](https://arxiv.org/abs/2212.03533)  
  → 약지도 대조 사전훈련 기반 텍스트 임베딩  
- 📄 [BGE: BAAI General Embedding (Xiao et al., 2023)](https://arxiv.org/abs/2309.07597)  
  → 범용 텍스트 임베딩 모델  
- 📄 [Matryoshka Representation Learning (Kusupati et al., NeurIPS 2022)](https://arxiv.org/abs/2205.13147)  
  → 다차원 임베딩 표현 학습  

**⚡ Efficient AI & Model Optimization**
- 📄 [LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., ICLR 2022)](https://arxiv.org/abs/2106.09685)  
  → LLM 파인튜닝 효율화 기술  
- 📄 [DistilBERT: A distilled version of BERT (Sanh et al., 2019)](https://arxiv.org/abs/1910.01108)  
  → BERT 모델 경량화 기법  
- 📄 [MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler et al., CVPR 2018)](https://arxiv.org/abs/1801.04381)  
  → 경량 CNN 구조 이해 및 의료 AI 적용 실험  
- 📄 [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan et al., ICML 2019)](https://arxiv.org/abs/1905.11946)  
  → 모델 확장 전략 및 효율적 CNN 설계  
- 📄 [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference (Jacob et al., CVPR 2018)](https://arxiv.org/abs/1712.05877)  
  → 모델 양자화를 통한 추론 최적화  

**👁️ Computer Vision & Object Detection**
- 📄 [YOLOv8: Ultralytics Implementation (2023)](https://github.com/ultralytics/ultralytics)  
  → 객체 탐지 최신 모델 실전 적용  
- 📄 [ResNet: Deep Residual Learning for Image Recognition (He et al., CVPR 2016)](https://arxiv.org/abs/1512.03385)  
  → 잔차 연결을 통한 깊은 네트워크 학습  
- 📄 [Vision Transformer: An Image is Worth 16x16 Words (Dosovitskiy et al., ICLR 2021)](https://arxiv.org/abs/2010.11929)  
  → 이미지 처리에 Transformer 구조 적용  

**🎤 Speech & Multimodal AI**
- 📄 [OpenAI Whisper: Robust Speech Recognition via Large-Scale Weak Supervision (Radford et al., 2022)](https://openai.com/research/whisper)  
  → 대규모 음성 데이터 학습 기반 STT 모델  
- 📄 [Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., NeurIPS 2020)](https://arxiv.org/abs/2006.11477)  
  → 자기지도 학습 기반 음성 표현 학습  
- 📄 [CLIP: Learning Transferable Visual Representations from Natural Language Supervision (Radford et al., ICML 2021)](https://arxiv.org/abs/2103.00020)  
  → 시각-언어 멀티모달 학습 모델  

**📝 Instruction Following & Advanced Training**
- 📄 [Instruction Tuning with GPT Models (OpenAI, 2023)](https://openai.com/research/instruction-following)  
  → LLM의 Instruction 기반 학습 원리  
- 📄 [Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)](https://arxiv.org/abs/2212.08073)  
  → AI 피드백을 통한 안전한 모델 학습  
- 📄 [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., NeurIPS 2022)](https://arxiv.org/abs/2201.11903)  
  → 단계별 추론을 통한 LLM 성능 향상  
- 📄 [Prompt Engineering Guide (DAIR.AI)](https://github.com/dair-ai/Prompt-Engineering-Guide)  
  → Prompt 설계 이론과 실습 전략 정리  

---
